{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13188\n",
      "https://www.ebay.com/sch/i.html?_from=R40&_nkw=new+balance+57/40&_in_kw=4&_sacat=11450&_sop=12&_ipg=240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x2456206f7b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new balance 57/40\n",
      "new balance fresh foam roav\n",
      "new balance fresh foam roav\n",
      "new balance 327\n",
      "new balance 574\n",
      "new balance fresh foam roav\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 5740\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n",
      "new balance 1080\n"
     ]
    }
   ],
   "source": [
    "from fake_headers import Headers\n",
    "import requests\n",
    "import hashlib\n",
    "import re\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from crochet import setup, wait_for\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "import argparse\n",
    "import io\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "PATH_TO_CLASSES = \"./data/classes.csv\"\n",
    "PATH_TO_ALL_HASH = \"./data/allhash.csv\"\n",
    "classes = pd.read_csv(PATH_TO_CLASSES)\n",
    "model_request = []\n",
    "model_name = []\n",
    "\n",
    "# my_cookies = {'dp1':r'',\n",
    "#            'ns1':r'',\n",
    "#            's':r'',\n",
    "#            'JSESSIONID':r'',\n",
    "#            'ebay':r'',\n",
    "#            'nonsession':r''\n",
    "#            },\n",
    "\n",
    "# fill cookies with yours from ebay.com, set up United Kindom location before.\n",
    "\n",
    "for index, row in classes.iterrows():\n",
    "    if classes.at[index, \"brand\"] in classes.at[index, \"model\"]:\n",
    "        result = classes.at[index, \"model\"]\n",
    "    else:\n",
    "        result = classes.at[index, \"brand\"] + \" \" + classes.at[index, \"model\"]\n",
    "    url = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=4&_sacat=11450&_sop=12&_ipg=240'\n",
    "    url1 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=4&_sacat=11450&_sop=12&_ipg=240&_pgn=2'\n",
    "    url2 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=4&_sacat=11450&_sop=12&_ipg=240&_pgn=3'\n",
    "    url3 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=1&_sacat=11450&_sop=12&_ipg=240&_pgn=4'\n",
    "    \n",
    "    # after first one finished, uncoment this and run second version \n",
    "    # url = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240'\n",
    "    # url1 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&_pgn=2'\n",
    "    # url2 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&_pgn=3'\n",
    "    # url3 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&_pgn=4' \n",
    "    \n",
    "    # after secone one finished, uncoment this and run third version \n",
    "    # url = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&LH_Complete=1&LH_Sold=1&_ipg=240'\n",
    "    # url1 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&LH_Complete=1&LH_Sold=1&_pgn=2'\n",
    "    # url2 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&LH_Complete=1&LH_Sold=1&_pgn=3'\n",
    "    # url3 = f'https://www.ebay.com/sch/i.html?_from=R40&_nkw={\"+\".join(result.split())}&_in_kw=3&_sacat=11450&_sop=12&_ipg=240&LH_Complete=1&LH_Sold=1&_pgn=4' \n",
    "\n",
    "    model_request.append(url)\n",
    "    model_request.append(url1)\n",
    "    model_request.append(url2)\n",
    "    model_request.append(url3)\n",
    "\n",
    "    model_name.append(result)\n",
    "    model_name.append(result)\n",
    "    model_name.append(result)\n",
    "    model_name.append(result)\n",
    "    \n",
    "\n",
    "print(len(model_request))\n",
    "setup()\n",
    "seen_sites = set()\n",
    "\n",
    "\n",
    "main_df = pd.read_csv(PATH_TO_ALL_HASH) # Dataframe with test data with column avg hash\n",
    "main_df['class_id'] = -1\n",
    "class_df = pd.read_csv(PATH_TO_CLASSES) # Dataframe with column class_id and model\n",
    "    \n",
    "\n",
    "class EbaySpider(scrapy.Spider):\n",
    "    \"\"\"\n",
    "    Our spider for parse image from ebay and analyze avg_hash them, therefore\n",
    "    fill dataframe finded class_id.\n",
    "    \"\"\"\n",
    "    name = \"Serega\"\n",
    "    custom_settings = {\n",
    "        'CONCURRENT_REQUESTS': 32,\n",
    "    }\n",
    "    \n",
    "    # 1 stage: iterate on search url\n",
    "    def start_requests(self):\n",
    "        meta = zip(model_request[:1000], model_name[:1000])\n",
    "        for url, model in meta:\n",
    "            yield scrapy.Request(url=url, \n",
    "                                 callback=self.parse,\n",
    "                                #  cookies=my_cookies, # uncomment this after all first version of the run finished\n",
    "                                 cb_kwargs=dict(find_model=model))\n",
    "\n",
    "    # 2 stage: find page url boots and iterate\n",
    "    def parse(self, response, find_model):\n",
    "        LIMIT_ITER = 240 # num of urls we iterate in loops\n",
    "\n",
    "        links = response.xpath(r'//div[@class = \"srp-river-results clearfix\"]//@href').getall()\n",
    "        for quote in links[:LIMIT_ITER]: \n",
    "            yield scrapy.Request(url=quote + \"\", \n",
    "                                 callback=self.parse_hash,\n",
    "                                #  cookies=my_cookies,\n",
    "                                 cb_kwargs=dict(find_model=find_model))\n",
    "    \n",
    "    # 3 stage: iterate on image in page boots and analyze them average hash\n",
    "    def parse_hash(self, response, find_model):\n",
    "        global class_df\n",
    "        global main_df\n",
    "        tmp_model = response.xpath(r'//div[@class=\"ux-layout-section ux-layout-section--features\"]//span[@itemprop=\"model\"]//span[@class=\"ux-textspans\"]/text()').get()\n",
    "        tmp_model = tmp_model.lower()\n",
    "        if tmp_model != find_model: # new balance 57/40  ||| 57/40\n",
    "            if len(class_df[class_df.model == tmp_model]) > 0:\n",
    "                fin = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l500\\.jpg\", str(response.text))\n",
    "                fin1 = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l1600\\.jpg\", str(response.text))\n",
    "                fin2 = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l640\\.jpg\", str(response.text))\n",
    "                fin = (list(set(fin)))\n",
    "                fin1 = (list(set(fin1)))\n",
    "                fin2 = (list(set(fin2)))\n",
    "                fin.extend(list(set(fin1)))\n",
    "                fin.extend(list(set(fin2)))\n",
    "\n",
    "                session = requests.Session()\n",
    "                for img_url in list(set(fin)):\n",
    "                    try:\n",
    "                        img_data = session.get(img_url, timeout=20)\n",
    "                    except:\n",
    "                        headers = Headers(headers=True).generate()\n",
    "                        session.headers = headers\n",
    "                        img_data = session.get(img_url, timeout=20)\n",
    "\n",
    "\n",
    "                    img_data = img_data.content\n",
    "                    with Image.open(io.BytesIO(img_data)) as img:\n",
    "                        avg_hash = str(imagehash.average_hash(img, 24))\n",
    "                    \n",
    "                    main_df.loc[(main_df['avg'] == avg_hash), 'class_id'] = self.class_df[self.class_df.model == tmp_model].id.values[0]\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            print(tmp_model)\n",
    "            fin = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l500\\.jpg\", str(response.text))\n",
    "            fin1 = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l1600\\.jpg\", str(response.text))\n",
    "            fin2 = re.findall(r\"https:\\/\\/i\\.ebayimg\\.com\\/(?!thumbs\\/)images\\/g\\/[a-zA-Z0-9]*\\/s-l640\\.jpg\", str(response.text))\n",
    "\n",
    "            fin = (list(set(fin)))\n",
    "            fin1 = (list(set(fin1)))\n",
    "            fin2 = (list(set(fin2)))\n",
    "            fin.extend(list(set(fin1)))\n",
    "            fin.extend(list(set(fin2)))\n",
    "            session = requests.Session()\n",
    "            for img_url in list(set(fin)):\n",
    "                try:\n",
    "                    img_data = session.get(img_url, timeout=20)\n",
    "                except:\n",
    "                    headers = Headers(headers=True).generate()\n",
    "                    session.headers = headers\n",
    "                    img_data = session.get(img_url, timeout=20)\n",
    "\n",
    "                img_data = img_data.content\n",
    "                with Image.open(io.BytesIO(img_data)) as img:\n",
    "                    avg_hash = str(imagehash.average_hash(img, 24))\n",
    "                main_df.loc[(main_df['avg'] == avg_hash), 'class_id'] = class_df[class_df.model == tmp_model].id.values[0]\n",
    "        \n",
    "# @wait_for(10)\n",
    "\n",
    "def run_spider():\n",
    "    \"\"\"run spider\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(EbaySpider)\n",
    "    return d\n",
    "\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1      84562\n",
       " 5        167\n",
       " 273      151\n",
       " 4        114\n",
       " 237      107\n",
       "        ...  \n",
       " 124        2\n",
       " 196        1\n",
       " 288        1\n",
       " 159        1\n",
       " 245        1\n",
       "Name: class_id, Length: 112, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "main_df.class_id.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv(\"one_of_the_possible_fill.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
